{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96c5080a",
   "metadata": {},
   "source": [
    "## Init Project, install requierment and convert the raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14719051",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75609fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import petl as etl\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efb79a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load file label\n",
    "\n",
    "label1 = etl.fromtext('rumor_detection_acl2017/twitter15/label.txt')\n",
    "label1 = label1.capture('lines', '(.*):(.*)$', ['label', 'src_tweet_id'])\n",
    "label1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97f6c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load file label\n",
    "\n",
    "label2 = etl.fromtext('rumor_detection_acl2017/twitter16/label.txt')\n",
    "label2 = label2.capture('lines', '(.*):(.*)$', ['label', 'src_tweet_id'])\n",
    "label2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec6bfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load file source_tweets\n",
    "\n",
    "source_tweet1 = etl.fromtext('rumor_detection_acl2017/twitter15/source_tweets.txt')\n",
    "source_tweet1 = source_tweet1.capture('lines', '(.*)\\t(.*)$', ['src_tweet_id', 'content'])\n",
    "source_tweet1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5710e715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load file source_tweets\n",
    "\n",
    "source_tweet2 = etl.fromtext('rumor_detection_acl2017/twitter16/source_tweets.txt')\n",
    "source_tweet2 = source_tweet2.capture('lines', '(.*)\\t(.*)$', ['src_tweet_id', 'content'])\n",
    "source_tweet2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f81d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join table berdasarkan source tweet id\n",
    "# rename header sesuai dengan codingan asal\n",
    "\n",
    "tweet_text1 = etl.join(label1, source_tweet1, key='src_tweet_id')\n",
    "tweet_text1 = etl.cut(tweet_text1, 'label', 'content')\n",
    "tweet_text1 = etl.setheader(tweet_text1, ['category', 'text'])\n",
    "tweet_text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e32f8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text2 = etl.join(label2, source_tweet2, key='src_tweet_id')\n",
    "tweet_text2 = etl.cut(tweet_text2, 'label', 'content')\n",
    "tweet_text2 = etl.setheader(tweet_text2, ['category', 'text'])\n",
    "tweet_text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75270195",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# untuk mengetahui labelnya apa saja\n",
    "\n",
    "tweet_text1.distinct('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ac150e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text2.distinct('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3deff159",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tweet_text1))\n",
    "print(len(tweet_text2))\n",
    "len(tweet_text1)+len(tweet_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad12d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# menyimpan ke file csv\n",
    "\n",
    "etl.tocsv(tweet_text1, 'tweet-text-1.csv')\n",
    "etl.appendcsv(tweet_text2, 'tweet-text-1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f48b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text3 = etl.select(tweet_text1, \"{category} == 'true' or {category} == 'false'\")\n",
    "tweet_text3 = etl.convert(tweet_text3, 'category', 'replace', 'true', 'ya')\n",
    "tweet_text3 = etl.convert(tweet_text3, 'category', 'replace', 'false', 'tidak')\n",
    "\n",
    "tweet_text4 = etl.select(tweet_text2, \"{category} == 'true' or {category} == 'false'\")\n",
    "tweet_text4 = etl.convert(tweet_text4, 'category', 'replace', 'true', 'ya')\n",
    "tweet_text4 = etl.convert(tweet_text4, 'category', 'replace', 'false', 'tidak')\n",
    "\n",
    "etl.tocsv(tweet_text3, 'tweet-text-binary.csv')\n",
    "etl.appendcsv(tweet_text4, 'tweet-text-binary.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d57ed5",
   "metadata": {},
   "source": [
    "## Bert Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1212b546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tidak</td>\n",
       "      <td>deep-fried left wings demo-crab cakes barack-a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tidak</td>\n",
       "      <td>42 million dead in bloodiest black friday week...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tidak</td>\n",
       "      <td>42 million dead in bloodiest black friday week...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ya</td>\n",
       "      <td>#prayforchristopher 5k run .. well we are walk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tidak</td>\n",
       "      <td>a photo of black nurses saving the life of a k...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category                                               text\n",
       "0    tidak  deep-fried left wings demo-crab cakes barack-a...\n",
       "1    tidak  42 million dead in bloodiest black friday week...\n",
       "2    tidak  42 million dead in bloodiest black friday week...\n",
       "3       ya  #prayforchristopher 5k run .. well we are walk...\n",
       "4    tidak  a photo of black nurses saving the life of a k..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# datapath = 'bbc-text.csv'\n",
    "datapath = 'tweet-text-binary.csv'\n",
    "df = pd.read_csv(datapath)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20e2c678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='category'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.groupby(['category']).size().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73ed50db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "# labels = {'business':0,\n",
    "#           'entertainment':1,\n",
    "#           'sport':2,\n",
    "#           'tech':3,\n",
    "#           'politics':4\n",
    "#           }\n",
    "# labels = {'false':0,\n",
    "#           'non-rumor':1,\n",
    "#           'true':2,\n",
    "#           'unverified':3,\n",
    "#           }\n",
    "labels = {'tidak':0,\n",
    "          'ya':1,\n",
    "          }\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "\n",
    "        self.labels = [labels[label] for label in df['category']]\n",
    "        self.texts = [tokenizer(text, \n",
    "                               padding='max_length', max_length = 512, truncation=True,\n",
    "                                return_tensors=\"pt\") for text in df['text']]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb0e6d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "923 115 116\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(112)\n",
    "df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42), \n",
    "                                     [int(.8*len(df)), int(.9*len(df))])\n",
    "\n",
    "print(len(df_train),len(df_val), len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "809c5d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import BertModel\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.5):\n",
    "\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "#         self.linear = nn.Linear(768, 4)\n",
    "        self.linear = nn.Linear(768, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "\n",
    "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        final_layer = self.relu(linear_output)\n",
    "\n",
    "        return final_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6902b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, train_data, val_data, learning_rate, epochs):\n",
    "\n",
    "    train, val = Dataset(train_data), Dataset(val_data)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=2, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=2)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "            model = model.cuda()\n",
    "            criterion = criterion.cuda()\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "\n",
    "            total_acc_train = 0\n",
    "            total_loss_train = 0\n",
    "\n",
    "            for train_input, train_label in tqdm(train_dataloader):\n",
    "\n",
    "                train_label = train_label.to(device)\n",
    "                mask = train_input['attention_mask'].to(device)\n",
    "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                output = model(input_id, mask)\n",
    "                \n",
    "                batch_loss = criterion(output, train_label)\n",
    "                total_loss_train += batch_loss.item()\n",
    "                \n",
    "                acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "                total_acc_train += acc\n",
    "\n",
    "                model.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                for val_input, val_label in val_dataloader:\n",
    "\n",
    "                    val_label = val_label.to(device)\n",
    "                    mask = val_input['attention_mask'].to(device)\n",
    "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                    output = model(input_id, mask)\n",
    "\n",
    "                    batch_loss = criterion(output, val_label)\n",
    "                    total_loss_val += batch_loss.item()\n",
    "                    \n",
    "                    acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                    total_acc_val += acc\n",
    "            \n",
    "            print(\n",
    "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
    "                | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
    "                | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
    "                | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n",
    "                  \n",
    "EPOCHS = 20\n",
    "bert_model = BertClassifier()\n",
    "LR = 1e-6\n",
    "              \n",
    "train(model, df_train, df_val, LR, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e838784",
   "metadata": {},
   "source": [
    "## Save Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287cb06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model, open('fine-tuned-bert.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e91f2ab",
   "metadata": {},
   "source": [
    "## Load Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa09ead9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = pickle.load(open('fine-tuned-bert.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09ad0533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_data):\n",
    "\n",
    "    test = Dataset(test_data)\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=1)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "        model = model.cuda()\n",
    "\n",
    "    total_acc_test = 0\n",
    "    result = []\n",
    "    label_result = []\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for test_input, test_label in test_dataloader:\n",
    "\n",
    "              test_label = test_label.to(device)  \n",
    "              mask = test_input['attention_mask'].to(device)\n",
    "              input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "              output = model(input_id, mask)\n",
    "              result.append(output.cpu().numpy()[0])\n",
    "#               label_result.append(output.argmax(dim=1).cpu().numpy()[0])\n",
    "              label_result.append(test_label.cpu().numpy()[0])\n",
    "\n",
    "              acc = (output.argmax(dim=1) == test_label).sum().item()\n",
    "              \n",
    "              total_acc_test += acc\n",
    "    \n",
    "    print(f'Test Accuracy: {total_acc_test / len(test_data): .3f}')\n",
    "    return [np.array(result, dtype=np.float32), np.array(label_result, dtype=np.int8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a94c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(bert_model, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62cbcecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.969\n"
     ]
    }
   ],
   "source": [
    "vectorized_data, label_vectorized_data = evaluate(bert_model, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69ec3263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.5723724, 0.9174375],\n",
       "       [5.012532 , 0.       ],\n",
       "       [5.4816103, 0.       ],\n",
       "       ...,\n",
       "       [5.7759914, 0.       ],\n",
       "       [0.       , 5.796544 ],\n",
       "       [5.70356  , 0.       ]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c87a450a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 1, 0], dtype=int8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_vectorized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69d97806",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(vectorized_data, label_vectorized_data,\n",
    "                                                    stratify=label_vectorized_data, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d490b4fe",
   "metadata": {},
   "source": [
    "## MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e09bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "clf = MLPClassifier(random_state=1, hidden_layer_sizes=(4,), max_iter=1000).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaea1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1851ba84",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38244e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2699057",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cc8e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04909ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectorized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f05b79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "865/1154"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b186e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "289/1154"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae34f646",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.n_layers_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fd08c5",
   "metadata": {},
   "source": [
    "## CNN Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6566ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# Our dictionary will contain only of the top 7000 words appearing most frequently\n",
    "# top_words = 7000\n",
    "top_words = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046bac4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "# Now we split our data-set into training and test data\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "\n",
    "# Looking at the nature of training data\n",
    "print(X_train[0])\n",
    "print(y_train[0])\n",
    "\n",
    "print('Shape of training data: ')\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print('Shape of test data: ')\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec097316",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Padding the data samples to a maximum review length in words\n",
    "max_words = 450\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "\n",
    "# Building the CNN Model\n",
    "model = Sequential()      # initilaizing the Sequential nature for CNN model\n",
    "\n",
    "# Adding the embedding layer which will take in maximum of 450 words as\n",
    "# input and provide a 32 dimensional output of those words which belong in the top_words dictionary\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Conv1D(32, 3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa597584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 450, 32)           3200      \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 450, 32)           3104      \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 225, 32)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 7200)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 250)               1800250   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 251       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,806,805\n",
      "Trainable params: 1,806,805\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d86a4cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "7/7 - 0s - loss: 0.6975 - accuracy: 0.4960 - val_loss: 0.6925 - val_accuracy: 0.4983 - 346ms/epoch - 49ms/step\n",
      "Epoch 2/2\n",
      "7/7 - 0s - loss: 0.6921 - accuracy: 0.5445 - val_loss: 0.6907 - val_accuracy: 0.9689 - 32ms/epoch - 5ms/step\n",
      "Accuracy: 96.89%\n"
     ]
    }
   ],
   "source": [
    "# Fitting the data onto model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)# Getting score metrics from our model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)# Displays the accuracy of correct sentiment prediction over test data\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191195d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:textProcessBert]",
   "language": "python",
   "name": "conda-env-textProcessBert-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
