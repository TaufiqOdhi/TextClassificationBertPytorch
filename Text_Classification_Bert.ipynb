{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96c5080a",
   "metadata": {},
   "source": [
    "## Init Project, install requierment and convert the raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14719051",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75609fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import petl as etl\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efb79a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load file label\n",
    "\n",
    "label1 = etl.fromtext('rumor_detection_acl2017/twitter15/label.txt')\n",
    "label1 = label1.capture('lines', '(.*):(.*)$', ['label', 'src_tweet_id'])\n",
    "label1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97f6c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load file label\n",
    "\n",
    "label2 = etl.fromtext('rumor_detection_acl2017/twitter16/label.txt')\n",
    "label2 = label2.capture('lines', '(.*):(.*)$', ['label', 'src_tweet_id'])\n",
    "label2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec6bfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load file source_tweets\n",
    "\n",
    "source_tweet1 = etl.fromtext('rumor_detection_acl2017/twitter15/source_tweets.txt')\n",
    "source_tweet1 = source_tweet1.capture('lines', '(.*)\\t(.*)$', ['src_tweet_id', 'content'])\n",
    "source_tweet1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5710e715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load file source_tweets\n",
    "\n",
    "source_tweet2 = etl.fromtext('rumor_detection_acl2017/twitter16/source_tweets.txt')\n",
    "source_tweet2 = source_tweet2.capture('lines', '(.*)\\t(.*)$', ['src_tweet_id', 'content'])\n",
    "source_tweet2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f81d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join table berdasarkan source tweet id\n",
    "# rename header sesuai dengan codingan asal\n",
    "\n",
    "tweet_text1 = etl.join(label1, source_tweet1, key='src_tweet_id')\n",
    "tweet_text1 = etl.cut(tweet_text1, 'label', 'content')\n",
    "tweet_text1 = etl.setheader(tweet_text1, ['category', 'text'])\n",
    "tweet_text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e32f8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text2 = etl.join(label2, source_tweet2, key='src_tweet_id')\n",
    "tweet_text2 = etl.cut(tweet_text2, 'label', 'content')\n",
    "tweet_text2 = etl.setheader(tweet_text2, ['category', 'text'])\n",
    "tweet_text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75270195",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# untuk mengetahui labelnya apa saja\n",
    "\n",
    "tweet_text1.distinct('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ac150e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text2.distinct('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3deff159",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tweet_text1))\n",
    "print(len(tweet_text2))\n",
    "len(tweet_text1)+len(tweet_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad12d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# menyimpan ke file csv\n",
    "\n",
    "etl.tocsv(tweet_text1, 'tweet-text-1.csv')\n",
    "etl.appendcsv(tweet_text2, 'tweet-text-1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f48b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text3 = etl.select(tweet_text1, \"{category} == 'true' or {category} == 'false'\")\n",
    "tweet_text3 = etl.convert(tweet_text3, 'category', 'replace', 'true', 'ya')\n",
    "tweet_text3 = etl.convert(tweet_text3, 'category', 'replace', 'false', 'tidak')\n",
    "\n",
    "tweet_text4 = etl.select(tweet_text2, \"{category} == 'true' or {category} == 'false'\")\n",
    "tweet_text4 = etl.convert(tweet_text4, 'category', 'replace', 'true', 'ya')\n",
    "tweet_text4 = etl.convert(tweet_text4, 'category', 'replace', 'false', 'tidak')\n",
    "\n",
    "etl.tocsv(tweet_text3, 'tweet-text-binary.csv')\n",
    "etl.appendcsv(tweet_text4, 'tweet-text-binary.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d57ed5",
   "metadata": {},
   "source": [
    "## Bert Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1212b546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datapath = 'bbc-text.csv'\n",
    "datapath = 'tweet-text-binary.csv'\n",
    "df = pd.read_csv(datapath)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e2c678",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['category']).size().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ed50db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "# labels = {'business':0,\n",
    "#           'entertainment':1,\n",
    "#           'sport':2,\n",
    "#           'tech':3,\n",
    "#           'politics':4\n",
    "#           }\n",
    "# labels = {'false':0,\n",
    "#           'non-rumor':1,\n",
    "#           'true':2,\n",
    "#           'unverified':3,\n",
    "#           }\n",
    "labels = {'tidak':0,\n",
    "          'ya':1,\n",
    "          }\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "\n",
    "        self.labels = [labels[label] for label in df['category']]\n",
    "        self.texts = [tokenizer(text, \n",
    "                               padding='max_length', max_length = 512, truncation=True,\n",
    "                                return_tensors=\"pt\") for text in df['text']]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0e6d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(112)\n",
    "df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42), \n",
    "                                     [int(.8*len(df)), int(.9*len(df))])\n",
    "\n",
    "print(len(df_train),len(df_val), len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809c5d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import BertModel\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.5):\n",
    "\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "#         self.linear = nn.Linear(768, 4)\n",
    "        self.linear = nn.Linear(768, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "\n",
    "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        final_layer = self.relu(linear_output)\n",
    "\n",
    "        return final_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6902b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, train_data, val_data, learning_rate, epochs):\n",
    "\n",
    "    train, val = Dataset(train_data), Dataset(val_data)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=2, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=2)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "            model = model.cuda()\n",
    "            criterion = criterion.cuda()\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "\n",
    "            total_acc_train = 0\n",
    "            total_loss_train = 0\n",
    "\n",
    "            for train_input, train_label in tqdm(train_dataloader):\n",
    "\n",
    "                train_label = train_label.to(device)\n",
    "                mask = train_input['attention_mask'].to(device)\n",
    "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                output = model(input_id, mask)\n",
    "                \n",
    "                batch_loss = criterion(output, train_label)\n",
    "                total_loss_train += batch_loss.item()\n",
    "                \n",
    "                acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "                total_acc_train += acc\n",
    "\n",
    "                model.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                for val_input, val_label in val_dataloader:\n",
    "\n",
    "                    val_label = val_label.to(device)\n",
    "                    mask = val_input['attention_mask'].to(device)\n",
    "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                    output = model(input_id, mask)\n",
    "\n",
    "                    batch_loss = criterion(output, val_label)\n",
    "                    total_loss_val += batch_loss.item()\n",
    "                    \n",
    "                    acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                    total_acc_val += acc\n",
    "            \n",
    "            print(\n",
    "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
    "                | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
    "                | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
    "                | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n",
    "                  \n",
    "EPOCHS = 20\n",
    "bert_model = BertClassifier()\n",
    "LR = 1e-6\n",
    "              \n",
    "train(bert_model, df_train, df_val, LR, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e838784",
   "metadata": {},
   "source": [
    "## Save Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287cb06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model, open('fine-tuned-bert.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e91f2ab",
   "metadata": {},
   "source": [
    "## Load Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa09ead9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = pickle.load(open('fine-tuned-bert.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ad0533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_data):\n",
    "\n",
    "    test = Dataset(test_data)\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=1)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "        model = model.cuda()\n",
    "\n",
    "    total_acc_test = 0\n",
    "    result = []\n",
    "    label_result = []\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for test_input, test_label in test_dataloader:\n",
    "\n",
    "              test_label = test_label.to(device)  \n",
    "              mask = test_input['attention_mask'].to(device)\n",
    "              input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "              output = model(input_id, mask)\n",
    "              result.append(output.cpu().numpy()[0])\n",
    "#               label_result.append(output.argmax(dim=1).cpu().numpy()[0])\n",
    "              label_result.append(test_label.cpu().numpy()[0])\n",
    "\n",
    "              acc = (output.argmax(dim=1) == test_label).sum().item()\n",
    "              \n",
    "              total_acc_test += acc\n",
    "    \n",
    "    print(f'Test Accuracy: {total_acc_test / len(test_data): .3f}')\n",
    "    return [np.array(result, dtype=np.float32), np.array(label_result, dtype=np.int8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a94c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(bert_model, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cbcecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_data, label_vectorized_data = evaluate(bert_model, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ec3263",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87a450a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_vectorized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d97806",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(vectorized_data, label_vectorized_data,\n",
    "                                                    stratify=label_vectorized_data, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d490b4fe",
   "metadata": {},
   "source": [
    "## MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e09bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "clf = MLPClassifier(random_state=1, hidden_layer_sizes=(4,), max_iter=1000).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaea1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1851ba84",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38244e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2699057",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cc8e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04909ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectorized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f05b79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "865/1154"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b186e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "289/1154"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae34f646",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.n_layers_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fd08c5",
   "metadata": {},
   "source": [
    "## CNN Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6566ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# Our dictionary will contain only of the top 7000 words appearing most frequently\n",
    "# top_words = 7000\n",
    "top_words = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046bac4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "# Now we split our data-set into training and test data\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "\n",
    "# Looking at the nature of training data\n",
    "print(X_train[0])\n",
    "print(y_train[0])\n",
    "\n",
    "print('Shape of training data: ')\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print('Shape of test data: ')\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec097316",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Padding the data samples to a maximum review length in words\n",
    "max_words = 450\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "\n",
    "# Building the CNN Model\n",
    "model = Sequential()      # initilaizing the Sequential nature for CNN model\n",
    "\n",
    "# Adding the embedding layer which will take in maximum of 450 words as\n",
    "# input and provide a 32 dimensional output of those words which belong in the top_words dictionary\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Conv1D(32, 3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa597584",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86a4cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the data onto model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)# Getting score metrics from our model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)# Displays the accuracy of correct sentiment prediction over test data\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191195d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('BertPytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "dda10c7c31ee7a4653071c4f35ad8ce6be94f02cdd86ae89ad3e63c0ca4a578b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
